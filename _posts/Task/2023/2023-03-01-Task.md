---
layout: post
title: '2023-03-task'
category: task
tag: post
---

# 2023-03-01

## Animator(Exit time, Interruption Source, Transition time)
1. has exit time이 활성화되면 이전 애니메이션 클립이 특정 구간까지 재생이되야 다음 애니메이션 클립으로 전이가 가능하다. 예를 들어, has exit time이 true이고 exit time이 0.7이면 이전 애니메이션의 클립에서 70%(0.7)이전까지 전이 조건을 만족해도 전이가 되지않으며 70%가 지나서야 전이가 가능하다. 

2. Interruption Source
State의 Transition link에서 설정할 수 있다. (연결선 말하는 거임) 해당 전이(Transition)이 중단될 수 있는 조건을 설정한다.  
  
    A->B로 전이가 발생할때 A와 연결된 상태들과 B와 연결된 상태들에 대해서 인터럽트 설정이 가능하다.
    current: A->B로 전이 발생중, A와 연결된 상태들의 전이 조건을 만족하면 현재 전이를 중단(Interrupt)하고 그 전이(A와 연결된 상태로의 전이)로 이동한다.   

    next state: A->B로 전이 발생중,B와 연결된 상태들의 전이 조건을 만족하면 현재 전이를 중단(Interrupt)하고 그 전이(B와 연결된 상태로의 전이)로 이동한다.

참고: <https://jinhomang.tistory.com/116>

1. Transition time은 전이할때 걸리는 시간을 의미한다. 보통 애니메이션 전이시 끊기지 않는 자연스러운 전이를 구현할때 사용하는데, 반대로 transition time을 0으로 두어 바로바로 다음 상태의 애니메이션을 재생하도록 할 수 있다. 

## Getting Animation State
현재 실행중인 애니메이션 상태(State)의 정보를 알아야할 때가 있다.

필요한 함수는 Animator.GetCurrentAnimatorStateInfo()이고 파라메터는 레이어값을 받는데, Base Layer밖에없으면 0을 넣어주면 된다.

반환 값인 AnimatorStateInfo에서 IsName(string name)이라는 함수에 현재 조사하고 싶은 애니메이션 상태 이름을 넣어주면 런타임에서 해당 애니메이션 상태가 재생시 true를 반환한다.

## Detecting Animation End Event
특정 애니메이션 클립에 Add Event라는 버튼이 있는데, 원하는 시점에 이벤트를 추가해주면 해당 이벤트가 실행된다. 따라서 클립 종료 시점에 이벤트를 추가하면 실제로 클립 종료시 스크립트의 함수가 실행되므로 애니메이션 종료 시점을 감지할 수 있다.


![](/asset/images/20230302001632.png)

![](/asset/images/20230302001658.png)

참고:<https://gamedev.stackexchange.com/questions/117423/unity-detect-animations-end>


## Changing Animation Speed
Animator의 speed를 바꿀 수도 있지만 특정 상태 또는 클립만 속도가 다르도록 바꾸고 싶을때가 있다. AnimationClip.length의 경우 애니메이션의 클립 길이만 알려주므로 속도 변경이 불가능하고 AnimationStateInfo의 경우 speed, speedMultiplier는 읽기 전용이므로 수정이 불가능하다. 따라서 Animator에 speedMultiplier를 추가하기 위해 파라메터 변수를 추가해주고 설정하면 된다.  

![](/asset/images/20230302004159.png)

참조: <https://answers.unity.com/questions/1361902/changing-speed-of-specific-animation-state-at-runt.html>


# 2023-03-08

1. AI 데이터 셋 다운로드
2. Pycharm 필요한 패키지 인터프리터에 설치 - scipy, matplotlib
3. Batch, Epoch 개념
4. VAE tacotron 리포지토리 튜토리얼 의미 분석
5. 체크 포인트 확인


파라메터를 어떻게 해놨는지(o)
체크포인트 구현해놨는지(o)
--output_directory=outdir --log_directory=logdir
batch_size=6
epoch 건드려야되는지

하는게 뭐냐 -> 지금 타코트론 모델 소스 구현은 되어있음
-> 음질 향상, waveglow -> hifi-gan으로 고쳐서 음질의 품질을 향상시키는 것이 목표
-> 코드 분석 

1. Prepare Datasets
2. Clone this repo: git clone https://github.com/jinhan/tacotron2-vae.git
3. CD into this repo: cd tacotron2-vae
4. Initialize submodule: git submodule init; git submodule update
5. Update .wav paths: sed -i -- 's,DUMMY,ljs_dataset_folder/wavs,g' filelists/*.txt
6. Install requirements pip install -r requirements.txt
7. Training: python train.py --output_directory=outdir --log_directory=logdir -- hparams=training_files='filelists/8. koemo_spk_emo_all_train.txt',validation_files='filelists/koemo_spk_emo_all_valid.txt',anneal_function='constant',batch_size=6
9. Monitoring: tensorboard --logdir=outdir/logdir --host=127.0.0.1
10. Training results (~ 250,000 steps)

여기에서 

json 파일 확인 결과 -> sampling rate 44100으로 나옴
sampling rate는 16000으로 변경해야함
-> hparams 수정

데이터셋의 PATH 수정 및 


# 2023-03-10

1. 사용중인 리포지토리를 clone 했었는데 일부 패키지가 requirement에 적혀있지않았음(torch)
2. tensorflow의 경우 버전을 명시하지않았는데, 실제로는 이 코드 주인은 1.x 버전을 사용했지만 현재 1.x 버전은 사용 불가능하므로
2.x 버전을 사용해야했음. 그러나 더이상 사용하지않는 api 모듈과 함수가 적혀있기에 바꿔줬음

=> tensorflow.contrib.training.hparams => AttrDict로 변경했음
3. 그리고 조교님의 도움을 받아 다화자 스피커, 다감정 임베딩을 추가하라고 하셨고 따라서 이에 대한 수정사항을 적용해야하는 것으로 보임
4. 조교님의 말씀으론, 한 화자의 목소리가 중요하기보다는 여러 화자의 데이터셋을 학습하는데 사용하고 중점적으로 학습시키는 것은
발음이라고 말하심
5. 이에 따라 txt 형식도 다시 추가를 진행했음


# 2023-03-14
1. 만약 사용자 정의 모듈 또는 패키지가 인식이 되지않으면 file - settings - project - project_structure - source 선택