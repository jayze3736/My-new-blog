---
layout: post
title: '2023-4월달 프로젝트 진행'
category: Project-plan
tag: Capstone
---

# train 단계에서 문제
현재 데이터 셋을 100GB 정도 크기로 사용하는 중인데, 해당 데이터 셋으로 train을 돌려봤는데 6일에 3 epoch가 도는 결과가 보였다.

원인 후보
1. 데이터 셋 크기가 너무 크다(하지만 교수님이 보시기에는 그렇게 큰 데이터 셋으로 보이지 않았다.)
2. GPU의 속도가 느리다?(서버의 CPU와 연관?)
3. 헤비한 모델일 경우 학습 속도가 느릴 수 있음 -> LSTM은 속도가 느린 셀? 타코트론은 그닥 헤비한 모델이 아님...?

# 04-03 할일
1. 데이터 셋 축소 -> 필요한 화자만 줄이기(의문: 원래 여러 화자로 학습하려고 했던 이유는 텍스트의 발음을 연습하기 위함이었는데 pre_trained 모델에 대해서 학습을 진행하면 실질적으로 학습하는 건 화자의 목소리 추가뿐인가?)

2. inference 추론 -> 지금 하던 학습에서 inference 또는 체크 포인트에서 inference


3. 파라메터 결정

- load_checkpoint_path: 체크 포인트를 불러올 경로
- epoch 수: fine tunning의 경우 현재 보유하지 않고있는 데이터셋도 필요한가? 왜냐하면 1 epoch이라는 건 전체 데이터 셋을 다 학습했을때를 의미 하는데, tuned model을 만드는데 원래 학습했던 데이터도 다시 필요한가?

4. fine_tune -> 체크 포인트를 불러와 학습


5. alignment 확인 ->

# 04-03 질문
의문: 원래 여러 화자로 학습하려고 했던 이유는 텍스트의 발음을 연습하기 위함이었는데
pre_trained 모델에 대해서 학습을 진행하면 실질적으로 학습하는 건 화자의 목소리 추가뿐인가?